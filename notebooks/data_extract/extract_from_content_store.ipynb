{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from lxml import etree\n",
    "from itertools import chain\n",
    "from lxml import html\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "CONFIG = os.getenv(\"CONFIG\")\n",
    "blacklist_path = os.path.join(CONFIG, 'document_types_excluded_from_the_topic_taxonomy.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "content_store_db = mongo_client[\"content_store\"]\n",
    "content_store_collection = content_store_db[\"content_items\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_excluded_document_types():\n",
    "    with open(blacklist_path, 'r') as f:\n",
    "        return yaml.safe_load(f)['document_types']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about',\n",
       " 'about_our_services',\n",
       " 'access_and_opening',\n",
       " 'business_support_finder',\n",
       " 'coming_soon']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLACKLIST_DOCUMENT_TYPES = get_excluded_document_types()\n",
    "BLACKLIST_DOCUMENT_TYPES[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_PROJECTION = {\n",
    "    \"expanded_links.organisations\": 1,\n",
    "    \"expanded_links.taxons\": 1,\n",
    "    \"expanded_links.pages_part_of_step_nav\":1,\n",
    "    \"expanded_links.primary_publishing_organisation\": 1,\n",
    "    \"expanded_links.worldwide_organisations\": 1,\n",
    "    \"expanded_links.supporting_organisations\": 1,\n",
    "    \"expanded_links.original_primary_publishing_organisation\": 1,\n",
    "    \"details.step_by_step_nav\":1,\n",
    "    \"details.body\": 1,\n",
    "    \"details.brand\": 1,  \n",
    "    \"details.documents\": 1,\n",
    "    \"details.final_outcome_detail\": 1,\n",
    "    \"details.final_outcome_documents\": 1,\n",
    "    \"details.government\": 1,\n",
    "    \"details.headers\": 1,\n",
    "    \"details.introduction\": 1,\n",
    "    \"details.introductory_paragraph\": 1,\n",
    "    \"details.licence_overview\": 1,\n",
    "    \"details.licence_short_description\": 1,\n",
    "    \"details.logo\": 1,\n",
    "    \"details.metadata\": 1,\n",
    "    \"details.more_information\": 1,\n",
    "    \"details.need_to_know\": 1,\n",
    "    \"details.other_ways_to_apply\": 1,\n",
    "    \"details.summary\": 1,\n",
    "    \"details.ways_to_respond\": 1,\n",
    "    \"details.what_you_need_to_know\": 1,\n",
    "    \"details.will_continue_on\": 1,\n",
    "    \"details.parts\": 1,\n",
    "    \"details.collection_groups\": 1,\n",
    "    \"details.transaction_start_link\": 1,\n",
    "    \"title\":1,\n",
    "    \"locale\":1,\n",
    "    \"description\":1,\n",
    "    \"document_type\": 1,\n",
    "    \"content_id\": 1}\n",
    "\n",
    "FILTER_BASIC = {\"$and\": [\n",
    "    {\"document_type\": {\"$nin\": BLACKLIST_DOCUMENT_TYPES}},\n",
    "    {\"phase\": \"live\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_items = content_store_collection.find(FILTER_BASIC, TEXT_PROJECTION)\n",
    "# # df = json_normalize(content_items)\n",
    "# rowlist = []\n",
    "# for i,item in enumerate(content_items):\n",
    "#     if i < 100:\n",
    "#         rowlist.append(item)\n",
    "#     else:\n",
    "#         break\n",
    "# df = pd.DataFrame(rowlist)\n",
    "# df\n",
    "# df = json_normalize(list(content_items)[0:10])\n",
    "# # df\n",
    "# df = pd.DataFrame([item for item in content_items][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test some stuff step by step fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgt = df[df.details.apply(lambda x: \"step_by_step_nav\" in x.keys())].iloc[0]\n",
    "# print(tgt.details.keys())\n",
    "# print([key for key in tgt.details['step_by_step_nav'].keys()])\n",
    "# tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tgt['details'].keys())\n",
    "# tgt['details'].pop('step_by_step_nav')\n",
    "# print(tgt['details'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgt = df[df.expanded_links.apply(lambda x: \"pages_part_of_step_nav\" in x.keys())].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for page in tgt.expanded_links['pages_part_of_step_nav']:\n",
    "#     print(page['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get taxon parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for links in df.expanded_links.values:\n",
    "#     if 'taxons' in links:\n",
    "#         for taxon in links['taxons']:\n",
    "#             for parent in taxon['links']['parent_taxons']:\n",
    "#                 get_top_parent(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a special case where taxons outside of topic taxonomy (world content) have no \n",
    "# parent_taxon, so they get assigned \"None\". Not great, but since we're excluding world content\n",
    "# for now, ignored\n",
    "def get_top_parent(taxon):\n",
    "    if 'links' in taxon.keys():\n",
    "        if 'parent_taxons' in taxon['links']:\n",
    "            return get_top_parent(taxon['links']['parent_taxons'][0])\n",
    "        elif 'root_taxon' in taxon['links']:\n",
    "            return {'title':taxon['title'], \n",
    "                    'content_id':taxon['content_id'], \n",
    "                    'base_path':taxon['base_path']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_org_id(data):\n",
    "    return {key:[v['content_id'] for v in value] for key,value in data.items()}\n",
    "\n",
    "def extract_org_title(data):\n",
    "    return {key:[v['title'] for v in value] for key,value in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_org_title(df.iloc[0].expanded_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_html(text):\n",
    "    \"\"\"\n",
    "    Checks whether text is html or not\n",
    "    :param text: string\n",
    "    :return: bool\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return bool(BeautifulSoup(text, \"html.parser\").find())\n",
    "    # might be fine to except all exceptions here, as it's a low-level function\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def extract_text_from_content_details(data):\n",
    "    \"\"\"\n",
    "    Recurses through lists and dicts to find html and then extract links BE VERY CAREFUL AND PASS IN LINKS, otherwise old links may persist in the list\n",
    "    :param data: This function can accept a nested list or dict, or string\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if type(data) == list:\n",
    "        return \"\".join(list(chain.from_iterable([\n",
    "            extract_text_from_content_details(item)\n",
    "            for item in data\n",
    "        ])))\n",
    "    elif type(data) == dict:\n",
    "        return extract_text_from_content_details(list(data.values()))\n",
    "    elif is_html(data):\n",
    "        return extract_text(data)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def extract_text(body):\n",
    "    \"\"\"\n",
    "    Extract text from html body\n",
    "    :param body: <str> containing html.\n",
    "    \"\"\"\n",
    "    # TODO: Tidy this up!\n",
    "    r = None\n",
    "    # body != \"\\n\" and\n",
    "    if body and body != \"\\n\" and not body.isspace():\n",
    "        try:\n",
    "            # print(\"this is\", body)\n",
    "            tree = etree.HTML(body)\n",
    "            r = tree.xpath('//text()')\n",
    "            r = ' '.join(r)\n",
    "            r = r.strip().replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "            r = r.replace('\\n', ' ').replace(',', ' ')\n",
    "            # r = r.lower()\n",
    "            r = ' '.join(r.split())\n",
    "        except ValueError:\n",
    "            print(\"exception @ extract:\", type(body), body)\n",
    "    if not r:\n",
    "        r = ' '\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_data(mongodb_collection):\n",
    "    \"\"\"\n",
    "    Queries a MongoDB collection, get specific fields from details using TEXT_PROJECTION, converts this cursor to a DataFrame, with all details fields in one list column\n",
    "    :param mongodb_collection:\n",
    "    :return: pandas DataFrame with: _id (base_path), content_id, and all_details list column\n",
    "    \"\"\"\n",
    "    content_items = mongodb_collection.find(FILTER_BASIC, TEXT_PROJECTION)\n",
    "    row_list = []\n",
    "    \n",
    "    for i,item in enumerate(content_items):\n",
    "#         if i < 100:\n",
    "        if 'step_by_step_nav' in item['details'].keys():\n",
    "            item['sbs_details'] = item['details']['step_by_step_nav']\n",
    "            item['details'].pop('step_by_step_nav')\n",
    "            \n",
    "        item['details'] = extract_text_from_content_details(item['details'])\n",
    "        item['description'] = item['description']['value'] \\\n",
    "                                    if 'description' in item.keys() else \"\"\n",
    "        \n",
    "        if 'expanded_links' in item.keys():\n",
    "            item['orgs_id'] = extract_org_id(item['expanded_links'])\n",
    "            item['orgs_title'] = extract_org_title(item['expanded_links'])\n",
    "\n",
    "            if 'taxons' in item['expanded_links']:\n",
    "                taxon_list = []\n",
    "                for taxon in item['expanded_links']['taxons']:\n",
    "                    taxon_list.append(get_top_parent(taxon))\n",
    "                item['taxons'] = taxon_list\n",
    "            \n",
    "            if 'pages_part_of_step_nav' in item['expanded_links']:\n",
    "                item['pages_part_of_step_nav'] = item['expanded_links']\\\n",
    "                                        ['pages_part_of_step_nav']\n",
    "\n",
    "            del item['expanded_links']\n",
    "\n",
    "        row_list.append(item)\n",
    "\n",
    "        if i % 20000==0:\n",
    "            print(datetime.datetime.now().strftime(\"%H:%M:%S\"),\":\",i)\n",
    "\n",
    "    return row_list\n",
    "\n",
    "def df_wrapper(mongodb_collection):\n",
    "    data_list = get_page_data(mongodb_collection)\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df.rename(columns={'_id':'base_path','details':'text'}, inplace=True)\n",
    "    return df[['base_path', 'content_id', 'title', 'description', \n",
    "               'document_type', 'orgs_id', 'orgs_title','sbs_details', 'pages_part_of_step_nav',\n",
    "               'text', 'taxons', 'locale']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:23:35 : 0\n",
      "15:24:30 : 20000\n",
      "15:24:57 : 40000\n",
      "15:25:35 : 60000\n",
      "15:26:41 : 80000\n",
      "15:27:25 : 100000\n",
      "15:28:12 : 120000\n"
     ]
    }
   ],
   "source": [
    "df = df_wrapper(content_store_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(DATA_DIR, \"preprocessed_content_store_june.csv.gz\"), \n",
    "          index=False, \n",
    "          compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.locale=='en'].to_csv(os.path.join(DATA_DIR, \"preprocessed_content_store_en_june.csv.gz\"), \n",
    "          index=False, \n",
    "          compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
