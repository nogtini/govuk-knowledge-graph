{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Role: https://www.gov.uk/api/content/government/ministers/attorney-general\n",
    "\n",
    "People: https://www.gov.uk/api/content/government/people/matthew-hancock and list of all people: https://www.gov.uk/api/content/government/people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.getenv(\"DATA_DIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract people-related content and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_html_links(text):\n",
    "    \"\"\"\n",
    "    Grab any GOV.UK domain-specific (people) links from page text.\n",
    "    :param text: Text within a details sub-section, refer to filtered for keys.\n",
    "    :return: list of links\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    try:\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        links = [link.get('href') for link in soup.findAll('a', href=True)]\n",
    "    except Exception:\n",
    "        print(\"error\")\n",
    "    return [l.replace(\"https://www.gov.uk/\", \"/\") for l in links\n",
    "            if l.startswith(\"/government/people\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## trial\n",
    "# \"https://www.gov.uk/government/people\"\n",
    "# url = requests.get(\"https://www.gov.uk/government/people\")\n",
    "# htmltext = url.text\n",
    "# int(\"https://www.gov.uk/government/people?page=2\".split(\"=\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at: 10\n",
      "at: 20\n",
      "at: 30\n",
      "at: 40\n",
      "at: 50\n",
      "at: 60\n",
      "at: 70\n"
     ]
    }
   ],
   "source": [
    "all_people = []\n",
    "ind = 1\n",
    "next_page = 1\n",
    "while ind == next_page:\n",
    "    target = \"https://www.gov.uk/government/people?page={}\".format(ind)\n",
    "    url = requests.get(target)\n",
    "    link_list = extract_html_links(url.text)\n",
    "    all_people.extend(link_list[0:-1])\n",
    "    ind+=1\n",
    "    next_page = int(link_list[-1].split(\"=\")[-1])\n",
    "    if ind%10==0:\n",
    "        print(f\"at index: {ind}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, \"people_urls.csv\"), \"w\") as write:\n",
    "    for p in all_people:\n",
    "        write.write(p+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use people list to call content api and store relevant content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at: 0\n",
      "at: 500\n"
     ]
    }
   ],
   "source": [
    "people_content = []\n",
    "not_found = []\n",
    "for i, people_url in enumerate(all_people):\n",
    "    try:\n",
    "        url = \"https://www.gov.uk/api/content\" + people_url\n",
    "        content_item = json.loads(urllib.request.urlopen(url).read())\n",
    "        people_content.append(content_item)\n",
    "    except Exception:\n",
    "        # logger.debug(\"Url \\'{}\\' not found\".format(url))\n",
    "        not_found.append(people_url)\n",
    "    if i%500==0:\n",
    "        print(f'at: {i}')\n",
    "        \n",
    "df = pd.DataFrame(people_content)\n",
    "df.to_csv(os.path.join(DATA_DIR, \"people_content_store.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(not_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Preprocess appointment data in people content store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
